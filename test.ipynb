{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('F:\\\\nmrprediction\\\\CSpre\\\\inmemory')\n",
    "from model import regression\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bmrb_seq_dataset import refdb_get_seq, refdb_find_shift, refdb_get_shift_re, refdb_get_cs_seq\n",
    "from pdb_bmrb_ali import align_bmrb_pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import esm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_test_re(file_path, atom_type):\n",
    "    bmrb_seq = refdb_get_seq(file_path)\n",
    "    s, e = refdb_find_shift(file_path)\n",
    "    cs_seq = refdb_get_cs_seq(file_path, s, e)\n",
    "    matched = align_bmrb_pdb(bmrb_seq, cs_seq)\n",
    "    shift, mask = refdb_get_shift_re(file_path, s, e, bmrb_seq, matched, atom_type)\n",
    "    if '_' not in bmrb_seq:\n",
    "        label= torch.tensor(shift)\n",
    "        mask = torch.tensor(mask)\n",
    "        label = torch.nn.functional.pad(label, (0, 512-label.shape[0]))\n",
    "        model_path = \"F:\\\\.cache\\\\torch\\\\hub\\\\checkpoints\\\\esm2_t33_650M_UR50D.pt\"\n",
    "        model, alphabet = esm.pretrained.load_model_and_alphabet(model_path)\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "        model.eval()\n",
    "        data = [(\"protein1\", bmrb_seq)]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        embedding = token_representations[:, 1:-1, :].squeeze()\n",
    "        padding_mask = torch.zeros(512).bool()\n",
    "        padding_mask[:embedding.shape[0]] = True\n",
    "        embedding = torch.nn.functional.pad(embedding, (0, 0, 0, 512 - embedding.shape[0]))\n",
    "        mask = torch.nn.functional.pad(mask, (0, 512 - mask.shape[0]), value=False)\n",
    "        model = regression(1280, 512, 8, 0.1)\n",
    "        # model = classification(1280, 512, 8, 0.1)\n",
    "        padding_mask = padding_mask.unsqueeze(0)\n",
    "        model_ckpt = {\"CA\":\"reg_ca.pth\", \"CB\":\"reg_cb.pth\", \"C\":\"reg_c.pth\", \"N\":\"reg_n.pth\", \"HA\":\"reg_ha.pth\", \"H\":\"reg_h.pth\"}\n",
    "        model.load_state_dict(\n",
    "            torch.load(\"F:\\\\nmrprediction\\\\CSpre\\\\inmemory\\\\best_model\\\\\"+model_ckpt[atom_type], map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "        out = model(embedding.unsqueeze(0), padding_mask)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        loss = loss_func(out.squeeze(2).squeeze(0)[mask], label[mask])\n",
    "        return math.sqrt(loss), out.squeeze(2).squeeze(0)[mask].detach(), label[mask].detach()\n",
    "    return False, False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

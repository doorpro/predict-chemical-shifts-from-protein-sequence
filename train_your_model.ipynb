{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "from torch.utils.data import TensorDataset\n",
    "from utils import refdb_find_shift, refdb_get_cs_seq, refdb_get_shift_re, refdb_get_seq, get_HA_shifts, get_shifts, shiftx_get_cs_seq, shiftx_get_shift_re\n",
    "from utils import align_bmrb_pdb\n",
    "import os\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from model import PLM_CS\n",
    "from torch.utils.data import random_split\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data process\n",
    "In the data processing process, the esm model is used in advance to convert the sequence to embeddings and saved as tensordataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the esm2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-32): 33 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "esm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(refdb_path, save_path, atom_type):\n",
    "    all_esm_vec = torch.zeros(1, 512, 1280)\n",
    "    all_label = torch.zeros((1, 512))\n",
    "    all_mask = torch.zeros((1, 512)).bool()\n",
    "    all_padding_mask = torch.zeros((1, 512)).bool()\n",
    "    for root, directories, files in os.walk(refdb_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            bmrb_seq = refdb_get_seq(file_path)\n",
    "            s, e = refdb_find_shift(file_path)\n",
    "            cs_seq = refdb_get_cs_seq(file_path, s, e)\n",
    "            matched = align_bmrb_pdb(bmrb_seq, cs_seq)\n",
    "            shift, mask = refdb_get_shift_re(file_path, s, e, bmrb_seq, matched, atom_type)\n",
    "            if '_' not in bmrb_seq and 0<len(bmrb_seq) < 512:\n",
    "                data = [(\"protein1\", bmrb_seq)]\n",
    "                batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "                with torch.no_grad():\n",
    "                    results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "                token_representations = results[\"representations\"][33]\n",
    "                embedding = token_representations[:, 1:-1, :].squeeze()\n",
    "                embedding = torch.nn.functional.pad(embedding, (0, 0, 0, 512 - embedding.shape[0]))\n",
    "                # padding the size of tensor from \"res*1280\" to 512*1280\n",
    "                label = torch.tensor(shift)\n",
    "                padding_mask = torch.zeros(512).bool()\n",
    "                padding_mask[:label.shape[0]] = True\n",
    "                label = torch.nn.functional.pad(label, (0, 512-label.shape[0]))\n",
    "                # padding the size of tensor from \"res\" to 512\n",
    "                mask = torch.tensor(mask)\n",
    "                mask = torch.nn.functional.pad(mask, (0, 512-mask.shape[0]), value=False)\n",
    "                if not torch.all(mask.eq(False)):\n",
    "                    all_esm_vec = torch.cat((all_esm_vec, embedding.unsqueeze(0)), dim=0)\n",
    "                    all_label = torch.cat((all_label, label.unsqueeze(0)), dim=0)\n",
    "                    all_mask = torch.cat((all_mask, mask.unsqueeze(0)), dim=0)\n",
    "                    all_padding_mask = torch.cat((all_padding_mask, padding_mask.unsqueeze(0)), dim=0)\n",
    "        all_esm_vec = all_esm_vec[1:, :, :]\n",
    "        all_label = all_label[1:, :]\n",
    "        all_mask = all_mask[1:, :]\n",
    "        all_padding_mask = all_padding_mask[1:, :]\n",
    "        dataset = TensorDataset(all_esm_vec, all_label, all_mask, all_padding_mask)\n",
    "        torch.save(dataset, save_path)\n",
    "        print(\"Data saved successfully, size of dataset is: \", all_esm_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tensordatasets of 6 atom types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22609\\AppData\\Local\\Temp\\ipykernel_34568\\2552082538.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(shift)\n",
      "C:\\Users\\22609\\AppData\\Local\\Temp\\ipykernel_34568\\2552082538.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atom_type \u001b[38;5;129;01min\u001b[39;00m atom_types:\n\u001b[0;32m      8\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m save_dir \u001b[38;5;241m+\u001b[39m atom_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mdata_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matom_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mdata_process\u001b[1;34m(refdb_path, save_path, atom_type)\u001b[0m\n\u001b[0;32m     16\u001b[0m batch_labels, batch_strs, batch_tokens \u001b[38;5;241m=\u001b[39m batch_converter(data)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mesm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m token_representations \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m33\u001b[39m]\n\u001b[0;32m     20\u001b[0m embedding \u001b[38;5;241m=\u001b[39m token_representations[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\esm\\model\\esm2.py:112\u001b[0m, in \u001b[0;36mESM2.forward\u001b[1;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[0;32m    109\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 112\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[0;32m    118\u001b[0m         hidden_representations[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\esm\\modules.py:125\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[1;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[0;32m    123\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(x)\n\u001b[1;32m--> 125\u001b[0m x, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m    136\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\esm\\multihead_attention.py:357\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrot_emb:\n\u001b[0;32m    355\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrot_emb(q, k)\n\u001b[1;32m--> 357\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m MultiheadAttention\u001b[38;5;241m.\u001b[39mapply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(attn_weights\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m [bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import extract_protein_sequence, refdb_find_shift, refdb_get_cs_seq, refdb_get_shift_re\n",
    "from utils import align_bmrb_pdb\n",
    "import os\n",
    "atom_types = [\"CA\",\"CB\",\"C\",\"N\",\"H\",\"HA\"]\n",
    "refdb_path = \"./dataset/RefDB_test_remove\"\n",
    "save_dir = \"./dataset/tensordataset/\"\n",
    "for atom_type in atom_types:\n",
    "    save_path = save_dir + atom_type + \".pt\"\n",
    "    data_process(refdb_path, save_path, atom_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# if you are using multi-GPU, you can use torch.cuda.manual_seed_all(seed) to set all seeds.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "sys.argv = ['train_your_model.ipynb', '--batchsize', '16', '--N', '6', '--dropout', '0.1', '--d_model', '512', '--d_vec', '1280', '--n_head', '8', '--shuffle', 'False', '--epoch', '20000', '--lr', '5e-4', '--device', 'cpu']\n",
    "\n",
    "parser.add_argument('--batchsize', type=int, default=16, help='Batch size for training')\n",
    "parser.add_argument('--N', type=int, default=6, help='number of encoder')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='qkv d-model dimension')\n",
    "parser.add_argument('--d_vec', type=int, default=1280, help='amino embedding dimension')\n",
    "parser.add_argument('--n_head', type=int, default=8, help='number of attention heads')\n",
    "parser.add_argument('--shuffle', type=bool, default=False, help='shuffle dataset')\n",
    "parser.add_argument('--epoch', type=int, default=20000, help='epoch time')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "parser.add_argument('--device', type=str, default=\"cpu\", help='learning rate')\n",
    "# Change if you have cuda devices\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, save_path):\n",
    "    model = PLM_CS(args.d_vec, args.d_model, args.n_head, args.dropout)\n",
    "    device = torch.device(args.device)\n",
    "    train_loss_all = []\n",
    "    val_loss_all = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.99), eps=1e-8,\n",
    "                                   weight_decay=0)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer.zero_grad()\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    model.to(device)\n",
    "    def init_weights1(model):\n",
    "        if isinstance(model, torch.nn.Linear):\n",
    "            torch.nn.init.kaiming_uniform(model.weight)\n",
    "\n",
    "    def init_weights_kaiming(model):\n",
    "        if hasattr(model, 'weight') and model.weight.dim() > 1:\n",
    "            torch.nn.init.kaiming_uniform(model.weight.data)\n",
    "\n",
    "    def init_weights_xavier(model):\n",
    "        if isinstance(model, torch.nn.MultiheadAttention):\n",
    "            torch.nn.init.xavier_uniform_(model.in_proj_weight)\n",
    "            torch.nn.init.xavier_uniform_(model.out_proj.weight)\n",
    "\n",
    "    model.apply(init_weights1)\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    val_size = len(data) - train_size\n",
    "    train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
    "    traindata_loader = DataLoader(train_dataset, batch_size=args.batchsize, shuffle=True)\n",
    "    valdata_loader = DataLoader(val_dataset, batch_size=args.batchsize, shuffle=True)\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        all_CA = 0\n",
    "        for i, batch in enumerate(traindata_loader):\n",
    "            mask, label, seq_vec, padding_mask = batch[2], batch[1], batch[0], batch[3]\n",
    "            mask = mask.to(device)\n",
    "            label = label.to(device)\n",
    "            seq_vec = seq_vec.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            out = model(seq_vec, padding_mask)\n",
    "            loss = torch.sqrt(loss_func(out.squeeze(2)[mask], label[mask]))\n",
    "            # out, log_sigma = model(seq_vec, padding_mask)\n",
    "            # sigma_normal = torch.sqrt(torch.mean(0.5*(torch.exp((-1)*log_sigma)) * (out.squeeze(2)[mask] - label[mask])**2 + 0.5*log_sigma))\n",
    "            all_CA += label[mask].shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = loss ** 2 * label[mask].shape[0]\n",
    "            epoch_loss += mse.detach().item()\n",
    "        return (epoch_loss / all_CA)\n",
    "\n",
    "    def val(epoch):\n",
    "        # model.train()\n",
    "        epoch_loss = 0\n",
    "        all_CA = 0\n",
    "        for i, batch in enumerate(valdata_loader):\n",
    "            mask, label, seq_vec, padding_mask = batch[2], batch[1], batch[0], batch[3]\n",
    "            mask = mask.to(device)\n",
    "            label = label.to(device)\n",
    "            seq_vec = seq_vec.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            out = model(seq_vec, padding_mask)\n",
    "            loss = loss_func(out.squeeze(2)[mask], label[mask])\n",
    "            all_CA += label[mask].shape[0]\n",
    "            loss = loss * label[mask].shape[0]\n",
    "            epoch_loss += loss.item()\n",
    "            rmse = math.sqrt(epoch_loss / all_CA)\n",
    "        return rmse\n",
    "\n",
    "    best_acc = 1.8\n",
    "    for epoch in range(0, args.epoch):\n",
    "        train_loss = train(epoch)\n",
    "        val_loss = val(epoch)\n",
    "        print(f'\\tepoch{epoch:.3f}Train Loss: {train_loss:.3f} | val_rmse: {val_loss:7.3f}')\n",
    "        train_loss_all.append(train_loss)\n",
    "        val_loss_all.append(val_loss)\n",
    "\n",
    "        if val_loss<best_acc:\n",
    "            sp = save_path + f\"epoch{epoch}_val{val_loss:.3f}.pth\"\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"accuracy\": val_loss,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(state, sp)\n",
    "            best_acc = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models of six atom types separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/tensordataset/CA.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/your_model_ckpt/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39matom_type\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# you can change your model save path\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset/tensordataset/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43matom_type\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m main(data, save_path)\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/tensordataset/CA.pt'"
     ]
    }
   ],
   "source": [
    "atom_types = [\"CA\", \"CB\", \"C\", \"N\", \"H\", \"HA\"]\n",
    "for atom_type in atom_types:\n",
    "    save_path = \"./dataset/your_model_ckpt/\"+atom_type+\".pt\"\n",
    "    # you can change your model save path\n",
    "    data = np.load(\"./dataset/tensordataset/\"+atom_type+\".pt\", allow_pickle=True)\n",
    "    main(data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_model(sequence, result_file_name):\n",
    "    df = {\"sequence\": list(sequence), \"CA\": [0]*len(sequence), \"CB\": [0]*len(sequence), \"C\": [0]*len(sequence), \"N\": [0]*len(sequence), \"H\": [0]*len(sequence), \"HA\": [0]*len(sequence)}\n",
    "    atom_types = [\"CA\", \"CB\", \"C\", \"N\", \"H\", \"HA\"]\n",
    "    pred_shifts = {}\n",
    "    for atom_type in atom_types:\n",
    "        model = PLM_CS(1280, 512, 8, 0.1)\n",
    "        data = [(\"protein1\", sequence)]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        with torch.no_grad():\n",
    "            results = esm_model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        embedding = token_representations[:, 1:-1, :].squeeze()\n",
    "        padding_mask = torch.zeros(512).bool()\n",
    "        padding_mask[:embedding.shape[0]] = True\n",
    "        embedding = torch.nn.functional.pad(embedding, (0, 0, 0, 512 - embedding.shape[0]))\n",
    "        mask = torch.tensor([True]*len(sequence))\n",
    "        mask = torch.nn.functional.pad(mask, (0, 512 - mask.shape[0]), value=False)\n",
    "        padding_mask = padding_mask.unsqueeze(0)\n",
    "\n",
    "        model = PLM_CS(1280, 512, 8, 0.1)\n",
    "        # model.load_state_dict(\n",
    "        #     torch.load(\"./dataset/your_model_ckpt/\"+atom_type+\".pt\", map_location=torch.device('cpu')))\n",
    "        model.load_state_dict(\n",
    "            torch.load(\"./plm-cs/ckpt/model_ckpt/reg_ca.pth\", map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "        out = model(embedding.unsqueeze(0), padding_mask)\n",
    "        pred = out.squeeze(2).squeeze(0)[mask]\n",
    "        df[atom_type] = pred.tolist()\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(\"./result/\"+str(result_file_name)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how to use your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22609\\AppData\\Local\\Temp\\ipykernel_34568\\954314467.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"./plm-cs/ckpt/model_ckpt/reg_ca.pth\", map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "your_model(sequence=\"MVKVYAPASSANMSVLIQDLM\", result_file_name=\"result\")\n",
    "# An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example of evaluating proteins in the shiftx test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_shiftxfile(file_path, atom_types):\n",
    "    bmrb_seq = refdb_get_seq(file_path)\n",
    "    s, e = refdb_find_shift(file_path)\n",
    "    cs_seq = shiftx_get_cs_seq(file_path, s, e)\n",
    "    matched = align_bmrb_pdb(bmrb_seq, cs_seq)\n",
    "    six_rmse = []\n",
    "    if '_' not in bmrb_seq:\n",
    "        print(shift)\n",
    "        df = {'CA_label':[], 'CA_pred':[], 'CB_label':[], 'CB_pred':[], 'C_label':[], 'C_pred':[], 'N_label':[], 'N_pred':[], 'HA_label':[], 'HA_pred':[], 'H_label':[], 'H_pred':[], }\n",
    "        for atom_type in atom_types:\n",
    "            shift, mask = shiftx_get_shift_re(file_path, s, e, bmrb_seq, matched, atom_type)\n",
    "            label= torch.tensor(shift)\n",
    "            mask = torch.tensor(mask)\n",
    "            label = torch.nn.functional.pad(label, (0, 512-label.shape[0]))\n",
    "            data = [(\"protein1\", bmrb_seq)]\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            with torch.no_grad():\n",
    "                results = esm_model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "            embedding = token_representations[:, 1:-1, :].squeeze()\n",
    "            padding_mask = torch.zeros(512).bool()\n",
    "            padding_mask[:embedding.shape[0]] = True\n",
    "            embedding = torch.nn.functional.pad(embedding, (0, 0, 0, 512 - embedding.shape[0]))\n",
    "            mask = torch.nn.functional.pad(mask, (0, 512 - mask.shape[0]), value=False)\n",
    "            model = PLM_CS(1280, 512, 8, 0.1)\n",
    "            # model = classification(1280, 512, 8, 0.1)\n",
    "            padding_mask = padding_mask.unsqueeze(0)\n",
    "            model.load_state_dict(\n",
    "                torch.load(\"./dataset/your_model_ckpt/\"+atom_type+\".pt\", map_location=torch.device('cpu')))\n",
    "            model.eval()\n",
    "            out = model(embedding.unsqueeze(0), padding_mask)\n",
    "            loss_func = torch.nn.MSELoss()\n",
    "            loss = loss_func(out.squeeze(2).squeeze(0)[mask], label[mask])\n",
    "            rmse = math.sqrt(loss.item())\n",
    "            a = out.squeeze(2).squeeze(0)[mask].detach().numpy()\n",
    "            b = label[mask].detach().numpy()\n",
    "            df[atom_type+'_pred'] = a\n",
    "            df[atom_type+'_label']= b\n",
    "            print(file_path + atom_type+\" Inference finished, rmse is: \", rmse)\n",
    "            six_rmse.append(rmse)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(\"./result/\"+str(file_path)+\".csv\")\n",
    "    return six_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test each shiftx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/shiftx_test_setA001_bmr4032.str.corr.pdbresno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m      9\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/shiftx_test_set\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(file)\n\u001b[1;32m---> 10\u001b[0m     six_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mtest_on_shiftxfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matom_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     all_ca_rmse\u001b[38;5;241m.\u001b[39mappend(six_rmse[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     12\u001b[0m     all_cb_rmse\u001b[38;5;241m.\u001b[39mappend(six_rmse[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mtest_on_shiftxfile\u001b[1;34m(file_path, atom_types)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_on_shiftxfile\u001b[39m(file_path, atom_types):\n\u001b[1;32m----> 2\u001b[0m     bmrb_seq \u001b[38;5;241m=\u001b[39m \u001b[43mrefdb_get_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     s, e \u001b[38;5;241m=\u001b[39m refdb_find_shift(file_path)\n\u001b[0;32m      4\u001b[0m     cs_seq \u001b[38;5;241m=\u001b[39m refdb_get_cs_seq(file_path, s, e)\n",
      "File \u001b[1;32md:\\git\\plm-cs\\predict-chemical-shifts-from-protein-sequence\\utils.py:43\u001b[0m, in \u001b[0;36mrefdb_get_seq\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     41\u001b[0m start_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m99999\u001b[39m\n\u001b[0;32m     42\u001b[0m bmrb_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m entry:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(entry):\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Mol_residue_sequence\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m line:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/shiftx_test_setA001_bmr4032.str.corr.pdbresno'"
     ]
    }
   ],
   "source": [
    "all_ca_rmse = []\n",
    "all_cb_rmse = []\n",
    "all_c_rmse = []\n",
    "all_n_rmse = []\n",
    "all_ha_rmse = []\n",
    "all_h_rmse = []\n",
    "for root, directories, files in os.walk(\"./dataset/shiftx_test_set\"):\n",
    "    for file in files:\n",
    "        file_path = \"./dataset/shiftx_test_set\" + str(file)\n",
    "        six_rmse = test_on_shiftxfile(file_path, atom_types)\n",
    "        all_ca_rmse.append(six_rmse[0])\n",
    "        all_cb_rmse.append(six_rmse[1])\n",
    "        all_c_rmse.append(six_rmse[2])\n",
    "        all_n_rmse.append(six_rmse[3])\n",
    "        all_ha_rmse.append(six_rmse[4])\n",
    "        all_h_rmse.append(six_rmse[5])\n",
    "print(\"CA_rmse: \", np.mean(all_ca_rmse))\n",
    "print(\"CB_rmse: \", np.mean(all_cb_rmse))\n",
    "print(\"C_rmse: \", np.mean(all_c_rmse))\n",
    "print(\"N_rmse: \", np.mean(all_n_rmse))\n",
    "print(\"HA_rmse: \", np.mean(all_ha_rmse))\n",
    "print(\"H_rmse: \", np.mean(all_h_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example of evaluating proteins in the solution_nmr_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_solutionnmr(file_path, atom_types):\n",
    "    bmrb_seq_list = extract_protein_sequence(file_path)\n",
    "    six_rmse = []\n",
    "    for i, bmrb_seq in enumerate(bmrb_seq_list):\n",
    "        if '_' not in bmrb_seq and len(bmrb_seq) < 512:\n",
    "            data = [(\"protein1\", bmrb_seq_list[i])]\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            with torch.no_grad():\n",
    "                results = esm_model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "            embedding = token_representations[:, 1:-1, :].squeeze()\n",
    "            embedding = torch.nn.functional.pad(embedding, (0, 0, 0, 512 - embedding.shape[0]))\n",
    "            model = PLM_CS(1280, 512, 8, 0.1)\n",
    "            df = {'CA_label':[], 'CA_pred':[], 'CB_label':[], 'CB_pred':[], 'C_label':[], 'C_pred':[], 'N_label':[], 'N_pred':[], 'HA_label':[], 'HA_pred':[], 'H_label':[], 'H_pred':[], }\n",
    "            for atom_type in atom_types:\n",
    "                if atom_type == \"HA\":\n",
    "                    shifts, masks = get_HA_shifts(file_path, \"HA\", bmrb_seq_list)\n",
    "                else:\n",
    "                    shifts, masks = get_shifts(file_path, atom_type, bmrb_seq_list)\n",
    "                label= torch.tensor(shifts[i])\n",
    "                mask = torch.tensor(masks[i])\n",
    "                padding_mask = torch.zeros(512).bool()\n",
    "                padding_mask[:label.shape[0]] = True\n",
    "                label = torch.nn.functional.pad(label, (0, 512 - label.shape[0]))\n",
    "                mask = torch.nn.functional.pad(mask, (0, 512 - mask.shape[0]), value=False)\n",
    "                padding_mask = padding_mask.unsqueeze(0)\n",
    "                model.load_state_dict(\n",
    "                    torch.load(\"F:\\\\nmrprediction\\\\CSpre\\\\inmemory\\\\best_model\\\\\" + atom_types[atom_type] , map_location=torch.device('cpu')))\n",
    "                model.eval()\n",
    "                out = model(embedding.unsqueeze(0), padding_mask)\n",
    "                loss_func = torch.nn.MSELoss()\n",
    "                loss = loss_func(out.squeeze(2).squeeze(0)[mask], label[mask])\n",
    "                rmse = math.sqrt(loss.item())\n",
    "                a = out.squeeze(2).squeeze(0)[mask].detach().numpy()\n",
    "                b = label[mask].detach().numpy()\n",
    "                df[atom_type+'_pred'] = a\n",
    "                df[atom_type+'_label']= b\n",
    "                print(file_path + atom_type+\" Inference finished, rmse is: \", rmse)\n",
    "                six_rmse.append(rmse)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(\"F:\\\\nmrprediction\\\\CSpre\\\\dataset\\\\all_infer\\\\\"+str(file_path)+\".csv\")\n",
    "    \n",
    "    return six_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test each solution_nmr_test_set file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ca_rmse = []\n",
    "all_cb_rmse = []\n",
    "all_c_rmse = []\n",
    "all_n_rmse = []\n",
    "all_ha_rmse = []\n",
    "all_h_rmse = []\n",
    "for root, directories, files in os.walk(\"./dataset/solution_nmr_test_set\"):\n",
    "    for file in files:\n",
    "        file_path = \"./dataset/solution_nmr_test_set\" + str(file)\n",
    "        six_rmse = test_on_solutionnmr(file_path, atom_types)\n",
    "        all_ca_rmse.append(six_rmse[0])\n",
    "        all_cb_rmse.append(six_rmse[1])\n",
    "        all_c_rmse.append(six_rmse[2])\n",
    "        all_n_rmse.append(six_rmse[3])\n",
    "        all_ha_rmse.append(six_rmse[4])\n",
    "        all_h_rmse.append(six_rmse[5])\n",
    "print(\"CA_rmse: \", np.mean(all_ca_rmse))\n",
    "print(\"CB_rmse: \", np.mean(all_cb_rmse))\n",
    "print(\"C_rmse: \", np.mean(all_c_rmse))\n",
    "print(\"N_rmse: \", np.mean(all_n_rmse))\n",
    "print(\"HA_rmse: \", np.mean(all_ha_rmse))\n",
    "print(\"H_rmse: \", np.mean(all_h_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
